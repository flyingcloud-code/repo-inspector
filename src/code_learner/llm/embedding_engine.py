"""
åµŒå…¥ç”Ÿæˆå¼•æ“å®ç°

ä½¿ç”¨jina-embeddings-v2-base-codeæ¨¡å‹è¿›è¡Œä»£ç å‘é‡åŒ–
æ”¯æŒrepoçº§åˆ«æ‰¹é‡å¤„ç†ä¼˜åŒ–
"""
import logging
from typing import List, Optional
from pathlib import Path

from sentence_transformers import SentenceTransformer
import numpy as np

from ..core.interfaces import IEmbeddingEngine
from ..core.data_models import Function, EmbeddingData, EmbeddingVector
from ..core.exceptions import ModelLoadError, EmbeddingError
from ..utils.logger import get_logger

logger = get_logger(__name__)


class JinaEmbeddingEngine(IEmbeddingEngine):
    """jina-embeddings-v2-base-codeåµŒå…¥å¼•æ“
    
    ä¸“é—¨é’ˆå¯¹ä»£ç ä¼˜åŒ–çš„åµŒå…¥æ¨¡å‹ï¼Œæ”¯æŒrepoçº§åˆ«æ‰¹é‡å¤„ç†
    """
    
    def __init__(self, cache_dir: Optional[str] = None):
        """åˆå§‹åŒ–åµŒå…¥å¼•æ“
        
        Args:
            cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•
        """
        self.model: Optional[SentenceTransformer] = None
        self.cache_dir = cache_dir or "~/.cache/torch/sentence_transformers/"
        self.model_name: Optional[str] = None
        
    def load_model(self, model_name: str) -> bool:
        """åŠ è½½åµŒå…¥æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼Œæ¨è "jinaai/jina-embeddings-v2-base-code"
            
        Returns:
            bool: åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info(f"æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {model_name}")
            
            # åŠ è½½sentence-transformersæ¨¡å‹
            self.model = SentenceTransformer(
                model_name,
                cache_folder=self.cache_dir
            )
            self.model_name = model_name
            
            # éªŒè¯æ¨¡å‹åŠ è½½
            test_embedding = self.model.encode("test")
            embedding_dim = len(test_embedding)
            
            logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
            logger.info(f"ğŸ“Š åµŒå…¥ç»´åº¦: {embedding_dim}")
            logger.info(f"ğŸ’¾ ç¼“å­˜ç›®å½•: {self.cache_dir}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise ModelLoadError(model_name, f"Failed to load model: {str(e)}")
    
    def encode_text(self, text: str) -> EmbeddingVector:
        """ç¼–ç æ–‡æœ¬ä¸ºå‘é‡
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            EmbeddingVector: å‘é‡åµŒå…¥
            
        Raises:
            ModelLoadError: æ¨¡å‹æœªåŠ è½½
            EmbeddingError: ç¼–ç å¤±è´¥
        """
        if not self.model:
            raise ModelLoadError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_model()")
        
        if not text.strip():
            raise EmbeddingError(text, "è¾“å…¥æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        
        try:
            # ä½¿ç”¨sentence-transformersç¼–ç 
            embedding = self.model.encode(text, convert_to_numpy=True)
            return embedding.tolist()
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬ç¼–ç å¤±è´¥: {e}")
            raise EmbeddingError(text, f"ç¼–ç å¤±è´¥: {str(e)}")
    
    def encode_function(self, function: Function) -> EmbeddingData:
        """ç¼–ç å‡½æ•°ä¸ºå‘é‡åµŒå…¥
        
        Args:
            function: å‡½æ•°å¯¹è±¡
            
        Returns:
            EmbeddingData: åµŒå…¥æ•°æ®
        """
        if not self.model:
            raise ModelLoadError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_model()")
        
        try:
            # æ„å»ºå‡½æ•°çš„æ–‡æ¡£æ–‡æœ¬
            doc_text = self._create_function_document(function)
            
            # ç”Ÿæˆå‘é‡åµŒå…¥
            embedding = self.encode_text(doc_text)
            
            # åˆ›å»ºåµŒå…¥æ•°æ®å¯¹è±¡
            embedding_data = EmbeddingData(
                id=f"{function.file_path}_{function.name}_{function.start_line}",
                text=doc_text,
                embedding=embedding,
                metadata={
                    "type": "function",
                    "file_path": function.file_path,
                    "function_name": function.name,
                    "start_line": function.start_line,
                    "end_line": function.end_line,
                    "code_length": len(function.code)
                }
            )
            
            return embedding_data
            
        except Exception as e:
            logger.error(f"å‡½æ•°ç¼–ç å¤±è´¥: {function.name} in {function.file_path}: {e}")
            raise EmbeddingError(function.code, f"å‡½æ•°ç¼–ç å¤±è´¥: {str(e)}")
    
    def encode_batch(self, texts: List[str]) -> List[EmbeddingVector]:
        """æ‰¹é‡ç¼–ç æ–‡æœ¬ - repoçº§åˆ«ä¼˜åŒ–
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            List[EmbeddingVector]: å‘é‡åˆ—è¡¨
        """
        if not self.model:
            raise ModelLoadError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_model()")
        
        if not texts:
            return []
        
        try:
            logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡ç¼–ç  {len(texts)} ä¸ªæ–‡æœ¬")
            
            # ä½¿ç”¨sentence-transformersçš„æ‰¹é‡ç¼–ç ä¼˜åŒ–
            # batch_size=32 æ˜¯ä¸€ä¸ªå¹³è¡¡å†…å­˜å’Œé€Ÿåº¦çš„é€‰æ‹©
            embeddings = self.model.encode(
                texts,
                batch_size=32,
                show_progress_bar=True,
                convert_to_numpy=True
            )
            
            # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
            embedding_list = [embedding.tolist() for embedding in embeddings]
            
            logger.info(f"âœ… æ‰¹é‡ç¼–ç å®Œæˆ: {len(embedding_list)} ä¸ªå‘é‡")
            return embedding_list
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡ç¼–ç å¤±è´¥: {e}")
            raise EmbeddingError(f"batch_{len(texts)}_texts", f"æ‰¹é‡ç¼–ç å¤±è´¥: {str(e)}")
    
    def _create_function_document(self, function: Function) -> str:
        """ä¸ºå‡½æ•°åˆ›å»ºæ–‡æ¡£æ–‡æœ¬
        
        Args:
            function: å‡½æ•°å¯¹è±¡
            
        Returns:
            str: æ ¼å¼åŒ–çš„æ–‡æ¡£æ–‡æœ¬
        """
        # åˆ›å»ºç»“æ„åŒ–çš„å‡½æ•°æ–‡æ¡£
        doc_parts = [
            f"æ–‡ä»¶è·¯å¾„: {function.file_path}",
            f"å‡½æ•°åç§°: {function.name}",
            f"è¡Œå·èŒƒå›´: {function.start_line}-{function.end_line}",
            "å‡½æ•°ä»£ç :",
            function.code.strip()
        ]
        
        return "\n".join(doc_parts)
    
    def get_model_info(self) -> dict:
        """è·å–æ¨¡å‹ä¿¡æ¯
        
        Returns:
            dict: æ¨¡å‹ä¿¡æ¯
        """
        if not self.model:
            return {"status": "not_loaded"}
        
        # è·å–åµŒå…¥ç»´åº¦
        test_embedding = self.model.encode("test")
        
        return {
            "status": "loaded",
            "model_name": self.model_name,
            "embedding_dimension": len(test_embedding),
            "cache_dir": self.cache_dir,
            "device": str(self.model.device) if hasattr(self.model, 'device') else "cpu"
        } 
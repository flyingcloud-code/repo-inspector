"""
向量检索器实现

基于Chroma向量数据库的语义相似度检索。
"""

import logging
from typing import List, Dict, Any

from ..core.context_models import ContextItem, SourceType
from ..core.retriever_interfaces import IContextRetriever
from ..storage.chroma_store import ChromaVectorStore
from ..config.config_manager import ConfigManager
from ..llm.embedding_engine import JinaEmbeddingEngine
import os

logger = logging.getLogger(__name__)

class VectorContextRetriever(IContextRetriever):
    """
    Retrieves context by performing semantic search on a vector database.
    """
    def __init__(self):
        """Initializes the VectorContextRetriever."""
        self.config = ConfigManager()
        # Initialize embedding engine for similarity search
        self.embedding_engine = JinaEmbeddingEngine()
        # Initialize vector store with project isolation
        # Get project_id from environment for consistent collection naming
        project_id = os.getenv("CODE_LEARNER_PROJECT_ID")
        self.vector_store = ChromaVectorStore(project_id=project_id)
        logger.info("VectorContextRetriever initialized.")

    def get_source_type(self) -> SourceType:
        """Returns the source type of this retriever."""
        return SourceType.VECTOR

    def is_available(self) -> bool:
        """Checks if the vector store is available."""
        return self.vector_store.is_available()

    def retrieve(self, query: str, intent: Dict[str, Any]) -> List[ContextItem]:
        """
        Retrieves similar code snippets from the vector database.

        This method uses a multi-query strategy based on the analyzed intent
        to provide a broader and more relevant set of results.
        """
        config = self.config.get_config()
        retriever_top_k = config.enhanced_query.sources["vector"]["top_k"]
        sub_queries = self._generate_sub_queries(query, intent)
        
        all_results: List[Dict[str, Any]] = []
        for sub_query in sub_queries:
            try:
                results = self.vector_store.similarity_search(
                    query=sub_query,
                    top_k=max(3, retriever_top_k // 2),
                    collection_name=None,  # Use default collection with project isolation
                    embedding_engine=self.embedding_engine
                )
                all_results.extend(results)
            except Exception as e:
                logger.error(f"Vector search for sub-query '{sub_query}' failed: {e}")
                continue
            
        return self._deduplicate_and_convert(all_results, retriever_top_k)
    
    def _generate_sub_queries(self, query: str, intent: Dict[str, Any]) -> List[str]:
        """Generates multiple search queries based on the intent."""
        queries = {query}  # Use a set to handle duplicates automatically
        
        # Query based on the core intent and primary entity
        if intent.get("primary_entity") and intent.get("core_intent"):
            queries.add(f"{intent['primary_entity']} {intent['core_intent']}")
        
        # Use sub-queries generated by the intent analysis LLM
        if "sub_queries" in intent:
            for sq in intent["sub_queries"]:
                if sq.get("source") == "vector":
                    queries.add(sq["query"])
                    
        logger.info(f"Generated {len(queries)} sub-queries for vector search.")
        return list(queries)
    
    def _deduplicate_and_convert(self, results: List[Dict[str, Any]], top_k: int) -> List[ContextItem]:
        """Deduplicates search results and converts them to ContextItem objects."""
        seen_content = set()
        context_items = []
        
        # Sort by distance ascending (lower distance = higher similarity)
        results.sort(key=lambda x: x.get("distance", 1.0))
        
        for result in results:
            # Chroma returns 'document' instead of 'content'
            content = result.get("document")
            distance = result.get("distance", 1.0)
            # Convert distance to similarity score (1 - distance)
            score = max(0.0, 1.0 - distance)
            
            if not content or content in seen_content:
                continue
            
            seen_content.add(content)
            context_items.append(
                ContextItem(
                    content=content,
                    source="vector_search", 
                    score=score,
                    metadata=result.get("metadata", {})
                )
            )
            
            if len(context_items) >= top_k:
                break
        
        return context_items